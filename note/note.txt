

nobn loss居高不下 保持在110到130左右
bn在relu前 20save，loss在33到43，均值37左右，四角白9%，四角黑84%，三黑66%，三黑一白41%(???)，四白悬空65%
bn在relu后 50save，                         四角白14%，四角黑71%，三黑44%(???)，三黑一白33%(???)
对结算的规则（数子多少）有比较好的了解，但还是不够深刻
比如
[x, x, x, x, x, x, x, x],
[x, o, o, o, o, o, o, x],
[x, o, o, o, o, o, o, x],
[o, o, o, o, o, o, o, o],
[o, o, o, o, o, o, o, o],
[x, o, o, o, o, o, o, x],
[x, o, o, o, o, o, o, x],
[x, x, x, x, x, x, x, x]]
判断的黑棋胜率达到了84%


调个模型
现在收敛太快了，loss维持在1000
lr减小，sigmoid换tanh（zero的论文里是tanh），加深CNN
然后考虑上resnet

lr: 0.003 min_cost: 1020
lr: 0.001 min_cost: 1060
lr: 0.009 min_cost: 1030
lr的更改无太大意义
tanh没用

1.23
jb，基础不扎实，没意识到loss取mean和sum是完全不一样的，sum的学习率相当于mean的batchsize倍，我的batchsize一直是256，也就是说lr约为0.75，大到爆了。
改了以后发现resnet效果确实好，不过之前的实验结果白费了
momentum=0.9收敛不知道快了多少倍,第4代超过了没momentum的第23代
tanh确实比sigmoid收敛快那么一点点
tanh:						sigmoid * 4:
0: [1.8026668]				0: [1.510188]
1: [0.7825338]				1: [0.7934871]
2: [0.75040126]				2: [0.7542679]
3: [0.73562586]				3: [0.73684645]
4: [0.7249321]				4: [0.72723097]
5: [0.7194749]				5: [0.72312516]
6: [0.7136518]				6: [0.71688646]
7: [0.71161443]				7: [0.7154234]
8: [0.7049715]				8: [0.7131796]
9: [0.7061127]
10: [0.70104873]
11: [0.6994134]
可以看到tanh的loss仍然没有完全收敛，5res
1res 收敛的居然也不错 可能是vnet任务比较简单
momentum = 0.999原地爆炸

network_input[x, y, 0] = self.board[x, y] == -1 ...
也就是说不需要输入o棋子效果居然也还可以。。
神经网络真nm神奇

1.24 凌晨
得到了目前效果最好的一次测试，比预计的好太多
诸多优化方法表现出了他们的巨大作用，l2，bn，res缺一不可，收敛曲线非常优美。180代仍没有过拟合现象，model应该可以说确实理解了reversi。

截一下
3: 9.886002	0.75489354	3.9451668 # 分布基本随机
13: 9.002726	0.7167425	3.1613307
28: 8.575691	0.70082784	2.840983
36: 8.398759	0.69117665	2.7213159
44: 8.250599	0.69096583	2.620745 # 有点意思
54: 8.079334	0.6938613	2.5049047
... # vloss小幅震荡，总体上有极小幅下降，ploss基本呈线性下降，这是一个问题
160: 6.6610856	0.66839665	1.6884536 # 完全理解
176: 6.5381756	0.6656988	1.649105
179: 6.5073776	0.6648126	1.634224
184: 6.481653	0.660802	1.6372353
190: 6.445794	0.67378765	1.6181046
191: 6.4422464	0.6760389	1.6172347
192: 6.4345613	0.66460395	1.625716
193: 6.4181757	0.6675206	1.6113294

没法训练完了，噪声太大，还要睡觉，可以看到loss整体还在不断降低，现在降得很多是l2loss

通过结算规则实验感觉vnet还可以提高，pass大概在15%-30%，胜率依然在黑，（不过还是那句话，可能是样本的问题。。。，或者说这种根本达不到的局面，预测不准也正常）

现在的问题有l2loss占比太高，应该尝试调一下；lr可能太低，ploss线性下降太慢；res表现出了价值，层数应该如何提高一下；fc层有没有必要增加unit，phead多连一层；收敛太慢太慢，这样用MCT迭代的话不知道要搞多久，需要权衡一下时间与性能
这些搞完就只差个MCTS了，搞个模板应该不难，但还要考虑有没有必要cython一下

突然感觉一个问题，样本的policy是通过一个固定的概率分布（滤镜）生成的，也就是说p的复杂度很低，低层网络就完全能学到，那么用高res和低res训练出来的差别应该不大，因为太简单了。还是要试验一下

结果出来了，5res，lr=0.1 -> 0.01
125:	2.00319	0.67784	1.15704
126:	1.99237	0.66996	1.15529
127:	1.98386	0.66549	1.15240
15:	1.96789	0.68569	1.10804
16:	1.96137	0.67915	1.10865
17:	1.96995	0.69119	1.10575

3res, lr=0.03 -> 0.003
185:	2.39203	0.68004	1.25695
186:	2.41022	0.69726	1.26164
187:	2.38508	0.68760	1.24984
10:	2.35157	0.70279	1.20612
11:	2.35862	0.70794	1.20852
12:	2.35507	0.70485	1.20855
可以看到还是有区别的，ploss差了个0.1
3res从22:06 -> 25:00，耗时还是个大问题
明天正式开始搞MCTS

1.28
MCTS和并行gen已写完，再随便写个setup就可以迭代了
MCTS有个明显的特点就是如果这一步赢的概率较大，那MCT会扩展的比较深，如果输的概率比较大，会扩展的比较宽，不过这样的特点并不符合人类的思维方式
这个特点表现出来就是，AI认为自己会赢的时候会走得很稳健。
考虑Q = - son.Q - self.Q，表示平均的胜率的增量
测试一下哪个更强

最后几步会变快？前向传播数不变啊？

2.2
第五代
经过实验可以确认agent学会了reversi的规则，可以完美地判断可落子点，对于极端情况
[x, x, x, x, x, x, x, x],
[x, o, o, o, o, o, o, x],
[x, o, o, o, o, o, o, x],
[x, o, o, o, o, o, o, x],
[x, o, o, o, o, o, o, x],
[x, o, o, o, o, o, o, x],
[x, o, o, o, o, o, o, x],
[x, x, x, x, x, x, x, x]
给出的黑棋胜率为23%(-0.5388433)，已经能在最极端的情况下不错地判断胜负
经1.31的实验，l2参数对于防止过拟合，提升test上的准确率有非常神奇的效果，在此模型中测试了lamda=0.01, 0.003, 0.002, 0.001, 0.0007, 0.0001，测试结果表明0.001时效果最好，在一组数据上vloss最低达到了0.65，而0.001的版本vloss最低达到0.70，已经可以说是有质的不同。
按我目前的经验，l2参数从0向上调整时会经历两个阶段，第一阶段trainloss上升，testloss下降，第二阶段trainloss和testloss一起上升，这是因为l2太大导致模型不能很好的收敛。在第二阶段包括一二阶段的临界点，trainloss基本等于testloss。调整l2的方法的方法是观察trainloss和testloss的差，找到它变成0的临界点。

2.1的实验表明旋转对称数据增强可以有效降低vloss(0.65->0.62)，ploss不好观察(如果测试数据没有旋转对称不变性，那么ploss当然会上升)但据猜测降低幅度甚至超过vloss(第四代ploss=1.6 第五代ploss=1.05)。同时旋转对称数据增强后数据量大了八倍，这使得vhead在训练中没有出现任何过拟合现象。

1.31的实验表明在现阶段（数据质量和数据量）下，提升res层数对模型效果提升有限，对于第四代数据集，8192次对局，5res的最终vploss为2.27121，7res的最终vploss为2.26621.目前现一代对前一代的胜率在逐步下降，具体数据为
3 vs 2: 100%
4 vs 3: 90%
5 vs 4: 78%
可以发现模型效果收敛很快，现在考虑在训练第7代时将自对弈次数改为4096*3次并观察效果。

[[_, _, x, _, _, x, _, _],
 [_, _, x, _, _, x, _, x],
 [_, x, x, x, x, x, x, x],
 [x, x, x, o, x, x, x, x],
 [x, x, x, x, x, x, o, x],
 [x, x, x, x, x, x, x, x],
 [_, _, _, _, _, _, _, x],
 [_, _, _, _, _, _, _, _]])
不测不知道，一测吓一跳
这个明摆着白棋gg的局面，居然判断出了90%的胜率，这是vnet005，vnet004有80%，vnet003有65%，这个趋势明摆着。看ai的棋谱，还以为ai水平已经很高了我已经下不过了，结果一试才发现ai好tm白痴。现在的ai有一些bug，比如
[[_, _, _, _, _, _, _, _],
 [_, _, _, _, _, _, _, _],
 [_, _, x, x, x, x, _, _],
 [_, _, x, x, x, x, _, _],
 [_, _, x, x, x, x, _, _],
 [_, _, x, x, x, x, _, _],
 [_, _, _, _, _, _, _, _],
 [_, _, _, _, _, _, _, _]]
给出的x胜率居然只有10%！！！！
对局质量不够高？
也许我得收回我的话，上一个例子确实是模型有bug，但是考虑到对局根本不可能碰到所以emm。我发现模型喜欢防守反击，把自己的棋子包裹在对手的棋子内，让对方无路可走，只能走11之类的地方，然后ai再一举拿下。
_ _ o o o o _ _ 
_ o o o o o _ _ 
o o o o o x o o 
x x x o x x o _ 
o o o o o o o _ 
o o o o o o o _ 
_ _ o o x o _ _ 
_ _ o o o o _ _ 
---------------
Player: x
[0.9541262]
_ _ o o o o _ _ 
_ o o o o o _ _ 
o o o o o x o o 
x x x o x x o _ 
o x o x o o o _ 
o x x o o o o _ 
_ x x x x o _ _ 
_ _ o o o o _ _ 
---------------
白棋只剩两个可落子点，基本就是输了。
但是ai在128MCTS的时候确实很白痴。。这种套路并不能100%成功。
好吧，但是棋力还有很大的提升空间。