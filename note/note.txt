

nobn loss居高不下 保持在110到130左右
bn在relu前 20save，loss在33到43，均值37左右，四角白9%，四角黑84%，三黑66%，三黑一白41%(???)，四白悬空65%
bn在relu后 50save，                         四角白14%，四角黑71%，三黑44%(???)，三黑一白33%(???)
对结算的规则（数子多少）有比较好的了解，但还是不够深刻
比如
[x, x, x, x, x, x, x, x],
[x, o, o, o, o, o, o, x],
[x, o, o, o, o, o, o, x],
[o, o, o, o, o, o, o, o],
[o, o, o, o, o, o, o, o],
[x, o, o, o, o, o, o, x],
[x, o, o, o, o, o, o, x],
[x, x, x, x, x, x, x, x]]
判断的黑棋胜率达到了84%


调个模型
现在收敛太快了，loss维持在1000
lr减小，sigmoid换tanh（zero的论文里是tanh），加深CNN
然后考虑上resnet

lr: 0.003 min_cost: 1020
lr: 0.001 min_cost: 1060
lr: 0.009 min_cost: 1030
lr的更改无太大意义
tanh没用

1.23
jb，基础不扎实，没意识到loss取mean和sum是完全不一样的，sum的学习率相当于mean的batchsize倍，我的batchsize一直是256，也就是说lr约为0.75，大到爆了。
改了以后发现resnet效果确实好，不过之前的实验结果白费了
momentum=0.9收敛不知道快了多少倍,第4代超过了没momentum的第23代
tanh确实比sigmoid收敛快那么一点点
tanh:						sigmoid * 4:
0: [1.8026668]				0: [1.510188]
1: [0.7825338]				1: [0.7934871]
2: [0.75040126]				2: [0.7542679]
3: [0.73562586]				3: [0.73684645]
4: [0.7249321]				4: [0.72723097]
5: [0.7194749]				5: [0.72312516]
6: [0.7136518]				6: [0.71688646]
7: [0.71161443]				7: [0.7154234]
8: [0.7049715]				8: [0.7131796]
9: [0.7061127]
10: [0.70104873]
11: [0.6994134]
可以看到tanh的loss仍然没有完全收敛，5res
1res 收敛的居然也不错 可能是vnet任务比较简单
momentum = 0.999原地爆炸

network_input[x, y, 0] = self.board[x, y] == -1 ...
也就是说不需要输入o棋子效果居然也还可以。。
神经网络真nm神奇

加入phead后vhead确实效果会变好

1.24 凌晨
得到了目前效果最好的一次测试，比预计的好太多
诸多优化方法表现出了他们的巨大作用，l2，bn，res缺一不可，收敛曲线非常优美。180代仍没有过拟合现象，model应该可以说确实理解了reversi。

截一下
3: 9.886002	0.75489354	3.9451668 # 分布基本随机
13: 9.002726	0.7167425	3.1613307
28: 8.575691	0.70082784	2.840983
36: 8.398759	0.69117665	2.7213159
44: 8.250599	0.69096583	2.620745 # 有点意思
54: 8.079334	0.6938613	2.5049047
... # vloss小幅震荡，总体上有小幅下降，ploss基本呈线性下降，这是一个问题
160: 6.6610856	0.66839665	1.6884536 # 完全理解
176: 6.5381756	0.6656988	1.649105
179: 6.5073776	0.6648126	1.634224
184: 6.481653	0.660802	1.6372353
190: 6.445794	0.67378765	1.6181046
191: 6.4422464	0.6760389	1.6172347
192: 6.4345613	0.66460395	1.625716
193: 6.4181757	0.6675206	1.6113294

没法训练完了，噪声太大，还要睡觉，可以看到loss整体还在不断降低，现在降得很多是l2loss

通过结算规则实验感觉vnet还可以提高，pass大概在15%-30%，胜率依然在黑，（不过还是那句话，可能是样本的问题。。。，或者说这种根本达不到的局面，预测不准也正常）

现在的问题有l2loss占比太高，应该尝试调一下；lr可能太低，ploss线性下降太慢；res表现出了价值，层数应该如何提高一下；fc层有没有必要增加unit，phead多连一层；收敛太慢太慢，这样用MCT迭代的话不知道要搞多久，需要权衡一下时间与性能
这些搞完就只差个MCTS了，搞个模板应该不难，但还要考虑有没有必要cython一下

突然感觉一个问题，样本的policy是通过一个固定的概率分布（滤镜）生成的，也就是说p的复杂度很低，低层网络就完全能学到，那么用高res和低res训练出来的差别应该不大，因为太简单了。还是要试验一下

结果出来了，5res，lr=0.1 -> 0.01
125:	2.00319	0.67784	1.15704
126:	1.99237	0.66996	1.15529
127:	1.98386	0.66549	1.15240
15:	1.96789	0.68569	1.10804
16:	1.96137	0.67915	1.10865
17:	1.96995	0.69119	1.10575

3res, lr=0.03 -> 0.003
185:	2.39203	0.68004	1.25695
186:	2.41022	0.69726	1.26164
187:	2.38508	0.68760	1.24984
10:	2.35157	0.70279	1.20612
11:	2.35862	0.70794	1.20852
12:	2.35507	0.70485	1.20855
可以看到还是有区别的，ploss差了个0.1
3res从22:06 -> 25:00，耗时还是个大问题
明天正式开始搞MCTS

1.28
MCTS和并行gen已写完，再随便写个setup就可以迭代了
MCTS有个明显的特点就是如果这一步赢的概率较大，那MCT会扩展的比较深，如果输的概率比较大，会扩展的比较宽，不过这样的特点并不符合人类的思维方式
这个特点表现出来就是，AI认为自己会赢的时候会走得很稳健。
考虑Q = - son.Q - self.Q，表示平均的胜率的增量
测试一下哪个更强

最后几步会变快？前向传播数不变啊？

2.2
第五代
经过实验可以确认agent学会了reversi的规则，可以完美地判断可落子点，对于极端情况
[x, x, x, x, x, x, x, x],
[x, o, o, o, o, o, o, x],
[x, o, o, o, o, o, o, x],
[x, o, o, o, o, o, o, x],
[x, o, o, o, o, o, o, x],
[x, o, o, o, o, o, o, x],
[x, o, o, o, o, o, o, x],
[x, x, x, x, x, x, x, x]
给出的黑棋胜率为23%(-0.5388433)，已经能在最极端的情况下不错地判断胜负
经1.31的实验，l2参数对于防止过拟合，提升test上的准确率有非常神奇的效果，在此模型中测试了lamda=0.01, 0.003, 0.002, 0.001, 0.0007, 0.0001，测试结果表明0.001时效果最好，在一组数据上vloss最低达到了0.65，而0.001的版本vloss最低达到0.70，已经可以说是有质的不同。
按我目前的经验，l2参数从0向上调整时会经历两个阶段，第一阶段trainloss上升，testloss下降，第二阶段trainloss和testloss一起上升，这是因为l2太大导致模型不能很好的收敛。在第二阶段包括一二阶段的临界点，trainloss基本等于testloss。调整l2的方法的方法是观察trainloss和testloss的差，找到它变成0的临界点。

2.1的实验表明旋转对称数据增强可以有效降低vloss(0.65->0.62)，ploss不好观察(如果测试数据没有旋转对称不变性，那么ploss当然会上升)但据猜测降低幅度甚至超过vloss(第四代ploss=1.6 第五代ploss=1.05)。同时旋转对称数据增强后数据量大了八倍，这使得vhead在训练中没有出现任何过拟合现象。

1.31的实验表明在现阶段（数据质量和数据量）下，提升res层数对模型效果提升有限，对于第四代数据集，8192次对局，5res的最终vploss为2.27121，7res的最终vploss为2.26621.目前现一代对前一代的胜率在逐步下降，具体数据为
3 vs 2: 100%
4 vs 3: 90%
5 vs 4: 78%
可以发现模型效果收敛很快，现在考虑在训练第7代时将自对弈次数改为4096*3次并观察效果。

(ps:
6 vs 5: 60%
7 vs 6: 68% 这一代12288次自对弈，128模拟)

[[_, _, x, _, _, x, _, _],
 [_, _, x, _, _, x, _, x],
 [_, x, x, x, x, x, x, x],
 [x, x, x, o, x, x, x, x],
 [x, x, x, x, x, x, o, x],
 [x, x, x, x, x, x, x, x],
 [_, _, _, _, _, _, _, x],
 [_, _, _, _, _, _, _, _]])
不测不知道，一测吓一跳
这个明摆着白棋gg的局面，居然判断出了90%的胜率，这是vnet005，vnet004有80%，vnet003有65%，这个趋势明摆着。看ai的棋谱，还以为ai水平已经很高了我已经下不过了，结果一试才发现ai好tm白痴。现在的ai有一些bug，比如
[[_, _, _, _, _, _, _, _],
 [_, _, _, _, _, _, _, _],
 [_, _, x, x, x, x, _, _],
 [_, _, x, x, x, x, _, _],
 [_, _, x, x, x, x, _, _],
 [_, _, x, x, x, x, _, _],
 [_, _, _, _, _, _, _, _],
 [_, _, _, _, _, _, _, _]]
给出的x胜率居然只有10%！！！！
对局质量不够高？
也许我得收回我的话，上一个例子确实是模型有bug，但是考虑到对局根本不可能碰到所以emm。我发现模型喜欢防守反击，把自己的棋子包裹在对手的棋子内，让对方无路可走，只能走11之类的地方，然后ai再一举拿下。
_ _ o o o o _ _ 
_ o o o o o _ _ 
o o o o o x o o 
x x x o x x o _ 
o o o o o o o _ 
o o o o o o o _ 
_ _ o o x o _ _ 
_ _ o o o o _ _ 
---------------
0.09	0.21	0.00	0.00	0.00	0.00	0.00	0.01	
1.47	0.00	0.00	0.00	0.00	0.00	0.00	0.03	
0.00	0.00	0.00	0.00	0.00	0.00	0.00	0.01	
0.00	0.00	0.00	0.00	0.00	0.00	0.00	0.92	
0.00	0.00	0.00	0.00	0.00	0.00	0.01	52.90	
0.00	0.00	0.00	0.00	0.00	0.00	0.00	39.63	
0.55	1.05	0.00	0.00	0.00	0.01	0.72	1.74	
0.00	0.56	0.00	0.00	0.00	0.00	0.05	0.00	
pass: 0.00
--------------------------------
[0.98081064]
39 0.52896780 <MCTS.Node object at 0x000001BF48662EB8> 450 	-0.899211
47 0.39626208 <MCTS.Node object at 0x000001BF48785DD8> 474 	-0.918185
55 0.01739462 <MCTS.Node object at 0x000001BF487B2518> 56 	-0.634398
 8 0.01474173 <MCTS.Node object at 0x000001BF487B2400> 60 	-0.652150
49 0.01051896 <MCTS.Node object at 0x000001BF487B2828> 2018 	-0.975021
31 0.00915971 <MCTS.Node object at 0x000001BF487B24E0> 599 	-0.960578
54 0.00716229 <MCTS.Node object at 0x000001BF487B2470> 110 	-0.814229
57 0.00556359 <MCTS.Node object at 0x000001BF487B2630> 976 	-0.968075
48 0.00546827 <MCTS.Node object at 0x000001BF487B25C0> 82 	-0.763695
 1 0.00214624 <MCTS.Node object at 0x000001BF48856550> 69 	-0.721396
62 0.00046934 <MCTS.Node object at 0x000001BF487B2748> 35 	-0.476473
15 0.00034160 <MCTS.Node object at 0x000001BF487B2320> 70 	-0.727374
_ _ o o o o _ _ 
_ o o o o o _ _ 
o o o o o x o o 
x x x o x x o _ 
o x o x o o o _ 
o x x o o o o _ 
_ x x x x o _ _ 
_ _ o o o o _ _ 
---------------
白棋只剩两个可落子点，基本就是输了。
但是ai在128MCTS的时候确实很白痴。。这种套路并不能100%成功。
好吧，但是棋力还有很大的提升空间。

（其实这说明ai已经理解了黑白棋中凝聚的概念，是我不会下棋）


2.8 思考：
------------------------------------------------------------------
随机入场：相当于噪点，用随机的方法或者水平较差的agent生成前几步棋子，这样生成的走法不给net学习，随后由bestAgent接手，再正常进行对局和学习。由此加强的数据集的局面会丰富很多，可以增强局面评估的鲁棒性，猜测1/5左右的随机入场效果会不错。

2.9试一下
------------------------------------------------------------------
UCB估值有个很大的问题，考虑现在局面据模型评估有一方处于较大的优势，那如果模型功能良好的话，另一方应该会被评估为有较大的劣势。对于有优势的一方，UCB函数工作良好，可以在利用经验和探索新节点之间找到一个不错的平衡，但转移到劣势一方的时候，问题就出来了。
考虑根节点v值-0.5，那么每转移一个点，对方的v'都是0.5左右，我方所有探索过的子节点的Q都是负数，按UCB估值价值不如未探索过的点，这样会不停的探索新节点,无论多傻的节点都会去试，比如围棋中的1,1。即使探索完一遍还不算完，其实第二遍第三遍都有相同的结果，这个时候mcts基本就是宽搜，可以说非常傻了。
实际应用中1,1此类烂招肯定还会降低胜率即使胜率已经很低，比如25%到10%之类的，所以扩展一遍以后应该不会再访问，但是受限于我的机器的性能，一遍已经扩展不了。而且子节点应该会继续这么整，这个要试验一下。所以这样是不行的。
考虑加入截断函数，在到达一个点时，只有概率最大的k个点在考虑范围内，这个k并非常数，而是一个随该节点访问次数n而变化的函数，比如k=5+log2(n)。这样保证了不会去考虑那些垃圾点，而且随着访问次数增大会有越来越多的点纳入考虑范围。

记得实验：
实验结果写下来拍照放在手机里了。 2.8晚
总结一下，首先一点，我如果接下来要研究五子棋围棋，那这个优化非得有不可，不是闲得无聊胡思乱想，以五子棋为例，子局面是200这个数量级，如果在劣势中，我一百多的模拟次数还不够跑一遍的，那(温度)t=0和t=1又有啥区别？就是在子局面中随机一个。重复一遍，劣势一方大概率在所有可落子点中随机一个走，那还玩个P啊。
所以，受限于机器性能，这个算法必须要修改。在大劣不充分时，可以限制一下范围，但这个范围怎么限制，就按b+log(n)吗？b设为多少？而且在P很不良好时，限制步数确实会不利于策略增强。所以这个范围设置很重要。实验起来要花掉我太长的时间，我得考虑一下要不要把五子棋作为下一步的计划。
------------------------------------------------------------------
记得优化下chess.pyx (down)
内存爆炸了？试验一下root转移时有没有正确的内存释放 (down，没有正确释放)
------------------------------------------------------------------
预测获胜目数，保证胜率的情况下赢得尽量大，这样才有观赏性，不过估价函数中如何权衡胜率和目数，这个可能比较难处理。

这个可能永远也不会去实验了，毕竟其他更有价值的想法还有那么多，滑稽）
------------------------------------------------------------------
考虑斗地主类，非完全信息但是阶段性，状态非连续可以表示出来，那么网络不仅有估算自身转移概率的功能，还有估算对方转移概率的功能，当然还有胜率，这样就可以搜索了。在搜索树中，我方按正常UCB方法估值，对方则按概率直接跳转（或者用某种函数扭曲一下概率，比如再一开始增大方差，随后逐渐减小方差）。这样是不是就可以策略迭代了？

可能会把这个作为下一步目标。
------------------------------------------------------------------
autoencoder实验不要忘了
------------------------------------------------------------------